{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "5b4e353a", "cell_type": "markdown", "source": "# 01 — Análise Exploratória de Dados (EDA) — Vendas\nObjetivos:\n- Carregar e padronizar o dataset mensal (itens × lojas × período).\n- Entender estrutura, valores faltantes, zeros e outliers.\n- Visão de sazonalidade e tendências por período/loja/item.\n- Salvar uma base **limpa** para os próximos notebooks.\n", "metadata": {}}, {"id": "783e6a03", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# ============================\n# 0) IMPORTS & CONFIG\n# ============================\nimport os, sys, gc, json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option(\"display.max_columns\", 120)\n\n# Caminhos possíveis para o dataset (ajuste se necessário)\nCANDIDATE_PATHS = [\n    \"./data/base_mensal.csv\",\n    \"./base_mensal.csv\",\n    \"/mnt/data/base_mensal.csv\"\n]\nOUTPUT_DIR = \"./outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Nome dos campos esperados (serão inferidos automaticamente)\nCOLUMN_ALIASES = {\n    \"date\": [\"year_month\",\"data\",\"periodo\",\"period\",\"mes_ano\",\"dt\",\"date\",\"ym\",\"month_year\"],\n    \"item\": [\"item\",\"sku\",\"produto\",\"product_id\",\"id_item\"],\n    \"store\": [\"store\",\"loja\",\"id_loja\",\"shop\",\"filial\"],\n    \"sales\": [\"sales\",\"vendas\",\"qtd_vendas\",\"valor_vendas\",\"demand\",\"y\"],\n    \"price\": [\"mean_price\",\"price\",\"preco\",\"preço\",\"avg_price\",\"unit_price\"],\n    \"promo\": [\"promo\",\"promocao\",\"promotion\",\"is_promo\",\"flag_promo\"],\n    \"category\": [\"category\",\"categoria\",\"cat\"],\n    \"department\": [\"department\",\"departamento\",\"dept\"],\n    \"region\": [\"region\",\"regiao\",\"região\",\"area\"]\n}\n\ndef _infer_col(cols, aliases):\n    cols_lower = {c.lower(): c for c in cols}\n    for a in aliases:\n        if a.lower() in cols_lower:\n            return cols_lower[a.lower()]\n    return None\n\ndef infer_columns(df):\n    mapping = {}\n    for logical, aliases in COLUMN_ALIASES.items():\n        col = _infer_col(df.columns, aliases)\n        mapping[logical] = col\n    # Campos obrigatórios mínimos\n    required = [\"date\",\"item\",\"store\",\"sales\"]\n    missing = [k for k in required if mapping.get(k) is None]\n    if missing:\n        raise ValueError(f\"Não encontrei colunas mínimas {missing}. Colunas disponíveis: {list(df.columns)}\")\n    return mapping\n\ndef try_read_csv(paths):\n    last_err = None\n    for p in paths:\n        if os.path.exists(p):\n            try:\n                df = pd.read_csv(p)\n                print(f\"[OK] Lido: {p} -> shape {df.shape}\")\n                return df, p\n            except Exception as e:\n                last_err = e\n    raise FileNotFoundError(f\"Não consegui ler o CSV nas opções {paths}. Último erro: {last_err}\")\n\ndf_raw, used_path = try_read_csv(CANDIDATE_PATHS)\nmapping = infer_columns(df_raw)\n\nDATE_COL  = mapping[\"date\"]\nITEM_COL  = mapping[\"item\"]\nSTORE_COL = mapping[\"store\"]\nSALES_COL = mapping[\"sales\"]\nPRICE_COL = mapping[\"price\"]        # pode ser None\nPROMO_COL = mapping[\"promo\"]        # pode ser None\nCAT_COL   = mapping[\"category\"]     # pode ser None\nDEPT_COL  = mapping[\"department\"]   # pode ser None\nREG_COL   = mapping[\"region\"]       # pode ser None\n\nprint(\"[MAPPING]\")\nprint(json.dumps(mapping, indent=2, ensure_ascii=False))\n\n# Parse datas (mensal por padrão). Tenta múltiplos formatos.\ndf = df_raw.copy()\ndef _to_period_month(x):\n    x = str(x)\n    for fmt in (\"%Y-%m\",\"%Y/%m\",\"%Y%m\",\"%d/%m/%Y\",\"%Y-%m-%d\"):\n        try:\n            return pd.to_datetime(x, format=fmt).to_period(\"M\").to_timestamp()\n        except Exception:\n            pass\n    # tentativa mais flexível\n    try:\n        return pd.to_datetime(x).to_period(\"M\").to_timestamp()\n    except Exception:\n        return pd.NaT\n\ndf[DATE_COL] = df[DATE_COL].apply(_to_period_month)\nif df[DATE_COL].isna().any():\n    n_bad = df[DATE_COL].isna().sum()\n    print(f\"[WARN] {n_bad} datas não puderam ser parseadas e serão removidas.\")\n    df = df.dropna(subset=[DATE_COL])\n\n# Tipos\ndf[ITEM_COL]  = df[ITEM_COL].astype(\"string\")\ndf[STORE_COL] = df[STORE_COL].astype(\"string\")\nif CAT_COL:  df[CAT_COL]  = df[CAT_COL].astype(\"string\")\nif DEPT_COL: df[DEPT_COL] = df[DEPT_COL].astype(\"string\")\nif REG_COL:  df[REG_COL]  = df[REG_COL].astype(\"string\")\n\n# Ordena\ndf = df.sort_values([DATE_COL, STORE_COL, ITEM_COL]).reset_index(drop=True)\n\nprint(df.head(3))\nprint(df.dtypes)\nprint(df.describe(include=\"all\").transpose().head(20))\n", "outputs": []}, {"id": "82356134", "cell_type": "markdown", "source": "## 1) Checagens básicas", "metadata": {}}, {"id": "b936de67", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Duplicados e linhas com NA em chaves\ndups = df.duplicated(subset=[DATE_COL, STORE_COL, ITEM_COL]).sum()\nprint(f\"[CHECK] Duplicados (date, store, item): {dups}\")\nna_keys = df[[DATE_COL, STORE_COL, ITEM_COL]].isna().any(axis=1).sum()\nprint(f\"[CHECK] NA em chaves: {na_keys}\")\n\n# Vendas zero e faltantes\nzero_sales = (df[SALES_COL] == 0).sum()\nna_sales   = df[SALES_COL].isna().sum()\nprint(f\"[CHECK] Vendas==0: {zero_sales} | Vendas NA: {na_sales}\")\n\n# Resumo de NA por coluna\nna_sum = df.isna().sum().sort_values(ascending=False)\ndisplay(na_sum.head(20))\n", "outputs": []}, {"id": "0640bb4a", "cell_type": "markdown", "source": "## 2) Visão geral de séries", "metadata": {}}, {"id": "2604d100", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Séries por loja e item\nn_stores = df[STORE_COL].nunique()\nn_items  = df[ITEM_COL].nunique()\nn_dates  = df[DATE_COL].nunique()\nprint(f\"[INFO] Lojas: {n_stores} | Itens: {n_items} | Meses: {n_dates} | Registros: {len(df)}\")\n\n# Totais mensais (toda a rede)\ntotals = df.groupby(DATE_COL, as_index=False)[SALES_COL].sum()\nfig = plt.figure(figsize=(10,4))\nplt.plot(totals[DATE_COL], totals[SALES_COL], marker=\"o\")\nplt.title(\"Vendas totais por mês\")\nplt.xlabel(\"Mês\"); plt.ylabel(\"Vendas\")\nplt.grid(True); plt.tight_layout(); plt.show()\n\n# Top lojas / itens por volume total\ntop_stores = df.groupby(STORE_COL)[SALES_COL].sum().sort_values(ascending=False).head(10)\ntop_items  = df.groupby(ITEM_COL)[SALES_COL].sum().sort_values(ascending=False).head(10)\ndisplay(top_stores.to_frame(\"total_vendas\"))\ndisplay(top_items.to_frame(\"total_vendas\"))\n", "outputs": []}, {"id": "71697665", "cell_type": "markdown", "source": "## 3) Sazonalidade (séries agregadas)", "metadata": {}}, {"id": "e19a747a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Sazonalidade média por mês do ano\ndf[\"_month\"] = df[DATE_COL].dt.month\nmonth_profile = df.groupby(\"_month\")[SALES_COL].mean()\nfig = plt.figure(figsize=(8,4))\nplt.plot(month_profile.index, month_profile.values, marker=\"o\")\nplt.title(\"Perfil médio por mês (sazonalidade)\")\nplt.xlabel(\"Mês\"); plt.ylabel(\"Vendas médias\")\nplt.grid(True); plt.tight_layout(); plt.show()\n", "outputs": []}, {"id": "3eed37d1", "cell_type": "markdown", "source": "## 4) Correlações rápidas (numéricas)", "metadata": {}}, {"id": "6da40972", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport numpy as np\nnum_df = df.select_dtypes(include=[np.number]).copy()\nif num_df.shape[1] >= 2:\n    corr = num_df.corr(numeric_only=True)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(corr, annot=False)\n    plt.title(\"Matriz de correlação (numéricas)\")\n    plt.tight_layout(); plt.show()\nelse:\n    print(\"[INFO] Poucas variáveis numéricas para correlação.\")\n", "outputs": []}, {"id": "449027e3", "cell_type": "markdown", "source": "## 5) Salvar base limpa para os próximos notebooks", "metadata": {}}, {"id": "4af4d571", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nclean_path_parquet = os.path.join(OUTPUT_DIR, \"base_mensal_clean.parquet\")\ndf.to_parquet(clean_path_parquet, index=False)\nprint(f\"[OK] Salvo: {clean_path_parquet}\")\n", "outputs": []}]}