{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "7b7e882d", "cell_type": "markdown", "source": "# 02 — Clusterização de Séries (Item × Loja)\nObjetivos:\n- Criar **clusters** de séries parecidas (item×loja) com base em estatísticas de comportamento (média, variação, tendência, sazonalidade, zeros, preço).\n- Salvar o `cluster` e um **pipeline** (scaler + k-means) para reutilizar na previsão.\n", "metadata": {}}, {"id": "b94e552c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# ============================\n# 0) IMPORTS & CONFIG\n# ============================\nimport os, sys, gc, json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import MiniBatchKMeans\nimport joblib\n\npd.set_option(\"display.max_columns\", 120)\n\nINPUT_PATHS = [\n    \"./outputs/base_mensal_clean.parquet\",\n    \"./base_mensal.csv\",\n    \"/mnt/data/base_mensal.csv\"\n]\n\nOUTPUT_DIR = \"./outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Mesmos aliases do notebook 01\nCOLUMN_ALIASES = {\n    \"date\": [\"year_month\",\"data\",\"periodo\",\"period\",\"mes_ano\",\"dt\",\"date\",\"ym\",\"month_year\"],\n    \"item\": [\"item\",\"sku\",\"produto\",\"product_id\",\"id_item\"],\n    \"store\": [\"store\",\"loja\",\"id_loja\",\"shop\",\"filial\"],\n    \"sales\": [\"sales\",\"vendas\",\"qtd_vendas\",\"valor_vendas\",\"demand\",\"y\"],\n    \"price\": [\"mean_price\",\"price\",\"preco\",\"preço\",\"avg_price\",\"unit_price\"],\n}\n\ndef _infer_col(cols, aliases):\n    cols_lower = {c.lower(): c for c in cols}\n    for a in aliases:\n        if a.lower() in cols_lower:\n            return cols_lower[a.lower()]\n    return None\n\ndef infer_columns(df):\n    mapping = {}\n    for logical, aliases in COLUMN_ALIASES.items():\n        col = _infer_col(df.columns, aliases)\n        mapping[logical] = col\n    required = [\"date\",\"item\",\"store\",\"sales\"]\n    missing = [k for k in required if mapping.get(k) is None]\n    if missing:\n        raise ValueError(f\"Faltam colunas {missing}.\")\n    return mapping\n\ndef try_load(paths):\n    for p in paths:\n        if os.path.exists(p):\n            try:\n                if p.endswith(\".parquet\"):\n                    df = pd.read_parquet(p)\n                else:\n                    df = pd.read_csv(p)\n                print(f\"[OK] Lido: {p} -> shape {df.shape}\")\n                return df\n            except Exception as e:\n                print(f\"[WARN] Falha ao ler {p}: {e}\")\n    raise FileNotFoundError(f\"Não encontrei dataset em {paths}\")\n\n# Hiperparâmetros de cluster\nN_CLUSTERS = 5\nRANDOM_STATE = 42\nBATCH_SIZE = 2048\n", "outputs": []}, {"id": "c77014c0", "cell_type": "markdown", "source": "## 1) Carregar e preparar", "metadata": {}}, {"id": "50d914da", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndf = try_load(INPUT_PATHS)\nmapping = infer_columns(df)\n\nDATE_COL  = mapping[\"date\"]\nITEM_COL  = mapping[\"item\"]\nSTORE_COL = mapping[\"store\"]\nSALES_COL = mapping[\"sales\"]\nPRICE_COL = mapping[\"price\"]\n\n# Converter tipos básicos\ndf[DATE_COL] = pd.to_datetime(df[DATE_COL]).dt.to_period(\"M\").dt.to_timestamp()\ndf[ITEM_COL]  = df[ITEM_COL].astype(\"string\")\ndf[STORE_COL] = df[STORE_COL].astype(\"string\")\n\ndf = df.sort_values([STORE_COL, ITEM_COL, DATE_COL]).reset_index(drop=True)\nprint(df.head(3))\n", "outputs": []}, {"id": "4625ecad", "cell_type": "markdown", "source": "## 2) Features por série (item×loja)", "metadata": {}}, {"id": "ffcbc6e7", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# time index por série\ndf[\"_time_index\"] = df.groupby([STORE_COL, ITEM_COL]).cumcount()\n\ndef per_series_features(pdf):\n    y = pdf[SALES_COL].astype(float).values\n    t = pdf[\"_time_index\"].astype(float).values\n    feats = {}\n    feats[\"n_months\"] = len(pdf)\n    feats[\"mean_sales\"] = float(np.nanmean(y))\n    feats[\"std_sales\"]  = float(np.nanstd(y))\n    feats[\"cv_sales\"]   = float(feats[\"std_sales\"] / feats[\"mean_sales\"]) if feats[\"mean_sales\"]>0 else 0.0\n    feats[\"zero_rate\"]  = float((y==0).mean())\n    # tendência (reta) y ~ a + b*t\n    if len(t) >= 2 and np.nanstd(t) > 0:\n        b = np.polyfit(t, y, 1)[0]\n    else:\n        b = 0.0\n    feats[\"trend_slope\"] = float(b)\n    # sazonalidade ~ autocorrelação em 12 meses (se houver)\n    if len(y) > 12:\n        y12 = y[12:]\n        y_lag12 = y[:-12]\n        if np.std(y12) > 0 and np.std(y_lag12) > 0:\n            feats[\"acf12\"] = float(np.corrcoef(y12, y_lag12)[0,1])\n        else:\n            feats[\"acf12\"] = 0.0\n    else:\n        feats[\"acf12\"] = 0.0\n    # preço médio e correlação preço-venda (se houver)\n    if PRICE_COL and PRICE_COL in pdf.columns:\n        p = pdf[PRICE_COL].astype(float).values\n        feats[\"mean_price\"] = float(np.nanmean(p))\n        valid = (~np.isnan(y)) & (~np.isnan(p))\n        if valid.sum() > 3 and np.std(y[valid])>0 and np.std(p[valid])>0:\n            feats[\"corr_price_sales\"] = float(np.corrcoef(y[valid], p[valid])[0,1])\n        else:\n            feats[\"corr_price_sales\"] = 0.0\n    else:\n        feats[\"mean_price\"] = np.nan\n        feats[\"corr_price_sales\"] = np.nan\n    return pd.Series(feats)\n\nseries_feats = df.groupby([STORE_COL, ITEM_COL]).apply(per_series_features).reset_index()\nprint(series_feats.head())\n\n# Preenche NaN\nseries_feats = series_feats.fillna(0.0)\n", "outputs": []}, {"id": "d88e4591", "cell_type": "markdown", "source": "## 3) Padronizar e treinar MiniBatchKMeans", "metadata": {}}, {"id": "77ed4fd4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import MiniBatchKMeans\n\nfeature_cols = [\"n_months\",\"mean_sales\",\"std_sales\",\"cv_sales\",\"zero_rate\",\"trend_slope\",\"acf12\",\"mean_price\",\"corr_price_sales\"]\nX = series_feats[feature_cols].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, batch_size=BATCH_SIZE)\ncluster_labels = kmeans.fit_predict(X_scaled)\n\nseries_feats[\"cluster\"] = cluster_labels\nprint(series_feats[\"cluster\"].value_counts().sort_index())\n\n# Plot simples da distribuição por cluster\ncounts = series_feats[\"cluster\"].value_counts().sort_index()\nfig = plt.figure(figsize=(6,3))\nplt.bar(counts.index.astype(str), counts.values)\nplt.title(\"Distribuição de séries por cluster\")\nplt.xlabel(\"cluster\"); plt.ylabel(\"contagem\")\nplt.tight_layout(); plt.show()\n", "outputs": []}, {"id": "0d52df2d", "cell_type": "markdown", "source": "## 4) Salvar pipeline e anexar cluster ao DF completo", "metadata": {}}, {"id": "f6dcbe44", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport joblib, os\n# Salvar pipeline\npipe_path = os.path.join(OUTPUT_DIR, \"cluster_pipeline.pkl\")\njoblib.dump({\"scaler\": scaler, \"kmeans\": kmeans, \"features\": feature_cols}, pipe_path)\nprint(f\"[OK] Pipeline salvo em: {pipe_path}\")\n\n# Anexar cluster por (store,item)\ndf_cluster = df.merge(series_feats[[STORE_COL, ITEM_COL, \"cluster\"]], on=[STORE_COL, ITEM_COL], how=\"left\")\nout_parquet = os.path.join(OUTPUT_DIR, \"base_mensal_with_clusters.parquet\")\ndf_cluster.to_parquet(out_parquet, index=False)\nprint(f\"[OK] Base com clusters: {out_parquet}\")\n\n# Exportar features por cluster para análise\nfeat_csv = os.path.join(OUTPUT_DIR, \"series_features_by_cluster.csv\")\nseries_feats.to_csv(feat_csv, index=False)\nprint(f\"[OK] Features por série: {feat_csv}\")\n", "outputs": []}]}
